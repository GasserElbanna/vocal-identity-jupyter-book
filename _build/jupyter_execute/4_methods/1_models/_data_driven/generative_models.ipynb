{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task performed for generative models is to generate or reconstruct data with limited input information. For instance, reconstructing original audio from corrupted input, generating future samples from past inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different approaches and tasks that fall under the *Generative* category. One would be **Autoencoding** which is reconstructing the original input using an encoder and a decoder. Another task could be **Autoregressive Prediction** in which the model leverages past information to generate and predict future samples. Lastly, a model could learn to reconstruct masked samples fronm unmasked ones calling that task **Masked Reconstruction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, we used two self-supervised models that constitute the *Generative* family. We extracted the speech embeddings from **APC** and **TERA**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) [APC](https://arxiv.org/abs/1904.03240)\n",
    "The *Autoregressive Predictive Coding* model was trained on the * LibriSpeech* corpus and it was learning to predict the spectrum of a future frame based on past frames.\n",
    "\n",
    "The architecture of the model comprises a multi-layer unidirectional LSTM network with residual connections between two consecutive layers. The dimensions of the final layer is *512*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) [TERA](https://arxiv.org/abs/2007.06028)\n",
    "\n",
    "TERA model stands for *Transformer Encoder Representations from Alteration*. This model learns to reconstruct original frames from corrupted input. The input data is corrupted using *alteration*, whether altering time, frequency or magnitude. Then, teaching the model to reconstruct the original input. The model was trained on *LibriSpeech* dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the figure below, the altered frames are fed to the TERA encoder that comprises transformer encoders and the representations are fed to a prediction network to reconstruct the real frames.\n",
    "\n",
    "In our experiments, we used the pre-trained TERA encoder to generate the speech embeddings of size 512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ![tera.png](tera.png)|\n",
    "|:--:|\n",
    "| <b>Fig.1 - Architecture of TERA model (from *[TERA paper](https://arxiv.org/abs/2007.06028)*)</b>|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('serab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "993b4f41d035a9600f4a883796b4aed3b9d432fcb6f84c031ca1e7ebf948a22f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}